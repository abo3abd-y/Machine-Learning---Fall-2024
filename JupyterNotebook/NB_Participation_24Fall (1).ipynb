{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Participation: Naive Bayes\n",
        "\n",
        "This week, we will explore the ins and outs of Naive Bayes Classification. We'll be looking at different ways to tease apart the data, different forms of Naive Bayes Classification, and how they impact our results."
      ],
      "metadata": {
        "id": "X1ZNJvdxjhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import CategoricalNB"
      ],
      "metadata": {
        "id": "K2s76L5f9cJC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our work this week, we'll be exploring a classic ML dataset - the \"Adult\" data. The objective here is to predict a binary measure of income (above or below 50K) using features about the person."
      ],
      "metadata": {
        "id": "rEso512nmFXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iYFhp_On9YPY"
      },
      "outputs": [],
      "source": [
        "dataset = fetch_openml(name=\"adult\", version=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical NB\n",
        "\n",
        "For Categorical Naive Bayes, we need to work with features that can be properly treated as categories. However, the implementation of Categorical NB in SKLearn requires features to be transformed into numbers, [0, num_categories-1]. We use an OrdinalEncoder to get the variables in this setting.\n",
        "\n",
        "The classifier also cannot handle missing information / NaN values, so we need the OrdinalEncoder to include them as a new category. We use -1 as the placehoder for this new category, then we add 1 to every category to make the feature values start at 0 again.\n",
        "\n",
        "**0) Change the random state below to your unique 5 digit identifier from your BuffOne Card.**"
      ],
      "metadata": {
        "id": "XOCFMJSqlm23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Numeric Features\n",
        "catData = dataset.data.drop([\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\"], axis=1)\n",
        "\n",
        "#Encode Features\n",
        "feature_encoder = OrdinalEncoder(handle_unknown = \"use_encoded_value\", encoded_missing_value=-1, unknown_value=-1)\n",
        "#feature_encoder = OrdinalEncoder()\n",
        "feature_encoder.fit(catData)\n",
        "encoded_dataset_X = feature_encoder.transform(catData)\n",
        "encoded_dataset_X = encoded_dataset_X + 1\n",
        "\n",
        "#Create Train/Test splits for our data\n",
        "train_X, test_X, train_y, test_y = train_test_split(encoded_dataset_X, dataset.target.values, test_size = 0.3, random_state = 78578)"
      ],
      "metadata": {
        "id": "NgqaTWuelsHu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create, fit, and score the CategoricalNB Classifier on our data.\n",
        "\n",
        "**1) Report your Accuracy.**\n",
        "\n",
        "The accuracy was around $0.79$."
      ],
      "metadata": {
        "id": "_a8Vz19-qyIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb = CategoricalNB()\n",
        "nb.fit(train_X, train_y)\n",
        "#Use the NB score function to get your resulting accuracy\n",
        "nb.score(test_X, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB3sl5a-K_gV",
        "outputId": "14be80f1-1f24-4bf5-e948-9d1221c7af33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7904865897768375"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Classify using the same features (i.e. use the train_X, test_x, train_y, and test_y) on a KNN and a Random Forest (just like our previous participations, you can copy that previous code). Which ones do better? Worse?**\n",
        "\n",
        "First of all, I used KNN and tested different n_neighbors hyperparamaters, and I found that $n=15$ yields the highest accuracy of $0.81$.\n",
        "\n",
        "For the random forest, I tested different n_estimators and I found that $\\text{n_estimator} = 96$ yields the highest accuracy of $0.82$.\n",
        "\n",
        "Therefore, the worst was naive bayes which did $0.79$."
      ],
      "metadata": {
        "id": "Hl7GT4vxsk84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(We went ahead and set up the imports for you)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Create and run your classifiers.\n",
        "\n",
        "#knn\n",
        "\n",
        "\n",
        "knn_lst_scores = [0] * 31\n",
        "\n",
        "for i in range(1,31):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(train_X, train_y)\n",
        "    knn_lst_scores[i] = knn.score(test_X, test_y)\n",
        "knn_lst_scores = knn_lst_scores[1:]\n",
        "\n",
        "print(f\"The best hypterparater for knn (neighbors) with the highest accuracy is {knn_lst_scores.index(max(knn_lst_scores)) + 1} with an accuracy of {max(knn_lst_scores)}\")\n",
        "\n",
        "#rf\n",
        "\n",
        "\n",
        "rf_lst_scores = [0] * 201\n",
        "for i in range(50, 201):\n",
        "    rf = RandomForestClassifier(n_estimators=i)\n",
        "    rf.fit(train_X, train_y)\n",
        "    rf_lst_scores[i] = rf.score(test_X, test_y)\n",
        "rf_lst_scores = rf_lst_scores[50:]\n",
        "print(f\"The best hypterparater for rf (estimators) with the highest accuracy is {rf_lst_scores.index(max(rf_lst_scores)) + 50} with an accuracy of {max(rf_lst_scores)}\")"
      ],
      "metadata": {
        "id": "a6cLWGqMu_Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182316a7-2a7b-4002-ba7c-1ce22dc19bb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best hypterparater for knn (neighbors) with the highest accuracy is 15 with an accuracy of 0.8099365317682385\n",
            "The best hypterparater for rf (estimators) with the highest accuracy is 96 with an accuracy of 0.8203098341636524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ml1c4xlhmF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}