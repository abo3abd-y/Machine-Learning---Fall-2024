{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Participation: Feature Engineering\n",
        "\n",
        "Assignment built using SKLearn Samples:\n",
        " - https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html"
      ],
      "metadata": {
        "id": "ymgnm2KUHJFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this activity, we will be exploring a multi-faceted dataset and implementing regression. We'll look at using baked-in SKLearn tools to understand the dataset, transform it to suit our needs, and attempt regression with KNN and Random Forests."
      ],
      "metadata": {
        "id": "Mbu-Dh4pg6gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4dmofamDHE3i"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0) Use your unique random seed (last 5 digits of your BuffOne card) in the train_test_split**"
      ],
      "metadata": {
        "id": "7kNnvsI-jTng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our dataset\n",
        "\n",
        "db_data = load_diabetes(scaled=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(db_data.data, db_data.target, test_size = 0.3, random_state=78578)"
      ],
      "metadata": {
        "id": "IskpIEnbHHTn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freely explore the dataset to answer the following questions:\n",
        "\n",
        "**1) How many features are there? (Do not include \"target\", which we will be using as our outcome measure)**\n",
        "\n",
        "There are 8 freatures (excluding target or class).\n",
        "\n",
        "**2) For each feature, figure out whether it is numeric (either continuous and discrete), ordinal, or categorical (including binary, e.g. True/False). Report the breakdown of types for the features.**\n",
        "\n",
        "Preg (Number of times pregnant): numeric but discrete (only integer).\n",
        "\n",
        "Plas (Plasma glucose concentration a 2 hours in an oral glucose tolerance test): It is numeric but continuous (even though it is an integer, it could be defined as continuous because they can be calculated in decimals but rounded up or down to an integer). Since it is an integer, we could also say that it is discrete.\n",
        "\n",
        "Pres (Blood Pressure): numeric but continous (same logic as plas, it could be measured in decimals but it is rounded to integer). Since it is an integer, it can be considered as discrete.\n",
        "\n",
        "Skin (Skin Fold Thickness): Numeric but continuous (same logic as before). However, in this case, since it is an integer, we could say that it is discrete.\n",
        "\n",
        "Insu (Serum Insulin): Numeric but continous. Same logic as before because insulin levels can be calculated as continous, but since it is an integer, we could consider it as discrete.\n",
        "\n",
        "Mass: Numeric but continous. Measured in decimals.\n",
        "\n",
        "Pedi (pedigree): Numeric but continous since it is measured in decimals.\n",
        "\n",
        "Age: Numeric but continous since age can be measured in decimals. However, since it is stored integer, we can consider it to be discrete.\n",
        "\n",
        "**3) How many samples are in our train set? Our test set?**\n",
        "\n",
        "There are 309 samples in training set and 133 samples in test set."
      ],
      "metadata": {
        "id": "oyNNrcCLIHTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your exploration workspace\n",
        "from sklearn.datasets import fetch_openml\n",
        "features = fetch_openml(data_id=37, as_frame=True) #id from the website\n",
        "features.frame.info()\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "\n",
        "#HINT Explore the SKLearn fetch_openml documentation to learn more about getting the data: https://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_openml.html"
      ],
      "metadata": {
        "id": "LZDziagWIGEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d8a8b5-31ef-41e9-eb15-8aae19c7e9d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column  Non-Null Count  Dtype   \n",
            "---  ------  --------------  -----   \n",
            " 0   preg    768 non-null    int64   \n",
            " 1   plas    768 non-null    int64   \n",
            " 2   pres    768 non-null    int64   \n",
            " 3   skin    768 non-null    int64   \n",
            " 4   insu    768 non-null    int64   \n",
            " 5   mass    768 non-null    float64 \n",
            " 6   pedi    768 non-null    float64 \n",
            " 7   age     768 non-null    int64   \n",
            " 8   class   768 non-null    category\n",
            "dtypes: category(1), float64(2), int64(6)\n",
            "memory usage: 49.0 KB\n",
            "309\n",
            "133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we better understand the bike sharing problem, let's look at trying to solve it. First, we can run regression on the unaltered features and see how they perform. Remember, for regression we need a measure of distance between real and predicted - for today, we will use the Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "GnGJsj0QNW74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Regressor\n",
        "\n",
        "knn = KNeighborsRegressor()\n",
        "knn.fit(X_train, y_train)\n",
        "knn_pred = knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_pred)\n",
        "print(knn_mse)\n",
        "\n",
        "#TODO Display resulting mse"
      ],
      "metadata": {
        "id": "CF0KQn5cPH1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc3df6b-5d32-4316-fe6f-9ce962ff366f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5369.800902255639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest Regressor\n",
        "forest = RandomForestRegressor()\n",
        "forest.fit(X_train, y_train)\n",
        "forest_pred = forest.predict(X_test)\n",
        "forest_mse = mean_squared_error(y_test, forest_pred)\n",
        "print(forest_mse)\n",
        "#TODO Display resulting MSE"
      ],
      "metadata": {
        "id": "CIJWqAejIfpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca5a9ab-088d-48c4-972b-3b7dc35bc52a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3423.705415789474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Which regressor performs better on our dataset? What is its MSE?**\n",
        "\n",
        "The random forest regressor performs better because its MSE is smaller than the KNN regressor's MSE. Its MSE is 3423.7"
      ],
      "metadata": {
        "id": "82x-I6mmiZ8E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSspYWaSpINI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's implement Min-Max Scaling and Normalization for our features in X. Again, we'll use SKLearn's implementations to accomplish this task."
      ],
      "metadata": {
        "id": "AwDDu5L9PYx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Min Max Scaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Standard Scaler (Normalizes)\n",
        "\n",
        "normer = StandardScaler()\n",
        "normer.fit(X_train)\n",
        "X_train_normed = normer.transform(X_train)\n",
        "X_test_normed = normer.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "iY3o2kngR65a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) How does the KNN Regressor perform on each transformed feature set?**\n",
        "\n",
        "With the scaler, the MSE was $3464.97$. For the normer, the MSE was $3394.30$. Therefore, it performed better with the normer.\n",
        "\n",
        "**5) How does the Random Forest Regressor perform on each transformed feature set?**\n",
        "\n",
        "With the scaler, the MSE was $3556.78$. With the normer, the MSE was $3330.78$. Therefore, it performed better with the normer."
      ],
      "metadata": {
        "id": "9ANW9eKwrMnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR EXPLORATION SPACE\n",
        "knn_scaler = KNeighborsRegressor()\n",
        "knn_scaler.fit(X_train_scaled, y_train)\n",
        "knn_scaler_pred = knn_scaler.predict(X_test_scaled)\n",
        "knn_scaler_mse = mean_squared_error(y_test, knn_scaler_pred)\n",
        "print(knn_scaler_mse)\n",
        "\n",
        "knn_normer = KNeighborsRegressor()\n",
        "knn_normer.fit(X_train_normed, y_train)\n",
        "knn_normer_pred = knn_normer.predict(X_test_normed)\n",
        "knn_normer_mse = mean_squared_error(y_test, knn_normer_pred)\n",
        "print(knn_normer_mse)\n",
        "\n",
        "forest_scaler = RandomForestRegressor()\n",
        "forest_scaler.fit(X_train_scaled, y_train)\n",
        "forest_scaler_pred = forest_scaler.predict(X_test_scaled)\n",
        "forest_scaler_mse = mean_squared_error(y_test, forest_scaler_pred)\n",
        "print(forest_scaler_mse)\n",
        "\n",
        "forest_normer = RandomForestRegressor()\n",
        "forest_normer.fit(X_train_normed, y_train)\n",
        "forest_normer_pred = forest_normer.predict(X_test_normed)\n",
        "forest_normer_mse = mean_squared_error(y_test, forest_normer_pred)\n",
        "print(forest_normer_mse)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# HINT - replace \"X_train\" in the fit() and \"X_test\" in the predict()\n"
      ],
      "metadata": {
        "id": "WKC6VSWNrM4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acb1794-525b-4454-c6e1-348712dba021"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3464.9702255639095\n",
            "3394.298345864662\n",
            "3463.2381007518793\n",
            "3446.2006691729325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BONUS (50 Participation Points)\n",
        "\n",
        "Play with the different feature transformation and tune the Regressors' hyperparameters to get the best results you can on MSE. Report the results of your exploration - which model / hyperparameters and which feature transformation combined to get the best results?"
      ],
      "metadata": {
        "id": "LUHDaIbWR80W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR EXPLORATION HERE\n",
        "\n",
        "#scaler knn\n",
        "\n",
        "knn_list = [0] * 21\n",
        "\n",
        "for i in range(1,21):\n",
        "  knn_list[i] = KNeighborsRegressor(n_neighbors=i)\n",
        "  knn_list[i].fit(X_train_scaled, y_train)\n",
        "  knn_scaler_pred = knn_list[i].predict(X_test_scaled)\n",
        "  knn_scaler_mse = mean_squared_error(y_test, knn_scaler_pred)\n",
        "  knn_list[i] = knn_scaler_mse\n",
        "\n",
        "knn_list = knn_list[1:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#normer knn\n",
        "\n",
        "\n",
        "knn_normer = [0] * 21\n",
        "\n",
        "\n",
        "for i in range(1,21):\n",
        "  knn_normer[i] = KNeighborsRegressor(n_neighbors=i)\n",
        "  knn_normer[i].fit(X_train_normed, y_train) #train\n",
        "  knn_normer_pred = knn_normer[i].predict(X_test_normed) #predict\n",
        "  knn_normer_mse = mean_squared_error(y_test, knn_normer_pred) #error\n",
        "  knn_normer[i] = knn_normer_mse #append\n",
        "\n",
        "knn_normer = knn_normer[1:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#scaler forest\n",
        "\n",
        "forest_scaler = [0] * 300\n",
        "\n",
        "for i in range(1,300):\n",
        "  forest_scaler[i] = RandomForestRegressor(n_estimators=i)\n",
        "  forest_scaler[i].fit(X_train_scaled, y_train)\n",
        "  forest_scaler_pred = forest_scaler[i].predict(X_test_scaled)\n",
        "  forest_scaler_mse = mean_squared_error(y_test, forest_scaler_pred)\n",
        "  forest_scaler[i] = forest_scaler_mse\n",
        "\n",
        "forest_scaler = forest_scaler[1:]\n",
        "\n",
        "\n",
        "\n",
        "#normer forst\n",
        "\n",
        "forest_normer = [0] * 300\n",
        "\n",
        "for i in range(1,300):\n",
        "  forest_normer[i] = RandomForestRegressor(n_estimators=i)\n",
        "  forest_normer[i].fit(X_train_normed, y_train)\n",
        "  forest_normer_pred = forest_normer[i].predict(X_test_normed)\n",
        "  forest_normer_mse = mean_squared_error(y_test, forest_normer_pred)\n",
        "  forest_normer[i] = forest_normer_mse\n",
        "\n",
        "forest_normer = forest_normer[1:]\n",
        "\n",
        "\n",
        "print(f\"The best hyperparamter (n) for scaler KNN is {knn_list.index(min(knn_list))} with an MSE of {min(knn_list)}\")\n",
        "print(f\"The best hyperparamter (n) for normer KNN is {knn_normer.index(min(knn_normer))} with an MSE of {min(knn_normer)}\")\n",
        "print(f\"The best hyperparamter (n_estimator) for scaler forest is {forest_scaler.index(min(forest_scaler))} with an MSE of {min(forest_scaler)}\")\n",
        "print(f\"The best hyperparamter (n_estimator) for normer forest is {forest_normer.index(min(forest_normer))} with an MSE of {min(forest_normer)}\")\n",
        "\n",
        "\n",
        "print(f\"Therefore, the best model with the best hyperparamter comes from the feature transformation normer that was trained by forest with n_estimator {forest_normer.index(min(forest_normer))} with an with an MSE of {min(forest_normer)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ubOr3DGSSw6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aceceb9-c8fa-4a97-ec22-2c3f65615990"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best hyperparamter (n) for scaler KNN is 16 with an MSE of 3351.9017353071254\n",
            "The best hyperparamter (n) for normer KNN is 15 with an MSE of 3274.1811266447367\n",
            "The best hyperparamter (n_estimator) for scaler forest is 77 with an MSE of 3221.5451523285533\n",
            "The best hyperparamter (n_estimator) for normer forest is 12 with an MSE of 3225.258664412511\n",
            "Therefore, the best model with the best hyperparamter comes from the feature transformation normer that was trained by forest with n_estimator 12 with an with an MSE of 3225.258664412511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qeZkT7gUiEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}